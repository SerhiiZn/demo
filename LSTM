import numpy as np
from tqdm import tqdm
import math
import matplotlib.pyplot as plt

import torch
from torch.autograd import Variable
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from termcolor import cprint, colored as c

from bs4 import BeautifulSoup
import os
os.environ["PYTHONIOENCODING"] = "utf-8"
from IPython.display import clear_output


def get_content(fn):
    with open(fn, 'r') as f:
        source = ""
        for line in f:
            source += line
    return source
    
def source_gen(path="get_data1/"):
    for child, folders, files in os.walk(path):
        for fn in files:
            if fn[0] is ".":
                pass
            else: 
                src = get_content(path + fn)
                soup = BeautifulSoup(src, 'html.parser')
                src = soup.getText()
                yield fn, src       


def source_gen1(path="get_data1/"):
    for child, folders, files in os.walk(path):
        for fn in files:
            if fn[0] is ".":
                pass
            else: 
                src = get_content(path + fn)
                soup = BeautifulSoup(src, 'html.parser')
                src = soup.getText()
                yield fn, src 

def batch_gen(seq_length, source):
    s_l = len(source)
    b_n = math.ceil(s_l/seq_length)
    s_pad = source + " " * (b_n * seq_length - s_l)
    for i in range(b_n):
        yield s_pad[i*seq_length: (i+1)*seq_length]


def num_flat_features(x):
    size = x.size()[1:]  # all dimensions except the batch dimension
    num_features = 1
    for s in size:
        num_features *= s
    return num_features


def forward_tracer(self, input, output):
    cprint(c("--> " + self.__class__.__name__, 'red') + " ===forward==> ")
  

def backward_tracer(self, input, output):
    cprint(c("--> " + self.__class__.__name__, 'red') + " ===backward==> ")


CHARS = "\x00 ABCDEFGHIJKLMNOPQRSTUVWXYZÆǾÅabcdefghijklmnopqrstuvwxyzæøå01234567890.,;:?\"'\n\r\t~!@#$%^&*()-/–—=_+<>{}[]|\\`~\xa0ëµ£"
CHAR_DICT = {ch: i for i, ch in enumerate(CHARS)}


class Char2Vec():
    def __init__(self, size=None, chars=None, add_unknown=False):
        if chars is None:
            self.chars = CHARS
        else:
            self.chars = chars
        self.char_dict = {ch: i for i, ch in enumerate(self.chars)}
        if size:
            self.size = size
        else:
            self.size = len(self.chars)
        if add_unknown:
            self.allow_unknown = True
            self.size += 1
            self.char_dict['<unk>'] = self.size - 1
        else:
            self.allow_unknown = False

    def get_ind(self, char):
        try:
            return self.char_dict[char]
        except KeyError:
            if self.allow_unknown is False:
                raise KeyError('character is not in dictionary: ' + str([char]))
            return self.char_dict['<unk>']

    def one_hot(self, source):
        y = torch.LongTensor([[self.get_ind(char)] for char in source])

        y_onehot = torch.zeros(len(source), self.size)
        y_onehot.scatter_(1, y, 1)

        return y_onehot

    def char_code(self, source):
        return torch.LongTensor([self.char_dict[char] for char in source])

    def vec2list(self, vec):
        chars = [self.chars[ind] for ind in vec.cpu().data.numpy()]
        return chars

def get_chars():
    step = 0
    freq = {}
    keys = []
    for file_name, source in tqdm(source_gen()):
        #print(source)
        try:
          for char in source:
              try:
                  freq[char] += 1
              except KeyError:
                  freq[char] = 1
                  keys.append(char)
        except: print('!')          
        #if step%10000 == 9999:
        #    print(str(step) + ": ln: " + str(len(keys)) + str(["".join(keys)]))
    
    return keys, freq



ks, freqs = get_chars()
print("".join(ks))

order = np.argsort([freqs[k] for k in ks])[::-1]
print(order)
chars_ordered = "".join(np.array([k for k in ks])[order])
print(chars_ordered[:140])

input_chars = list(" ABCDEFGHIJKLMNOPQRSTUVWXYZÆǾÂabcdefghijklmnopqrstuvwxyzæøå01234567890")
output_chars = ["<nop>", "<cap>"] + list(".,;:?!\"'")

class GruRNN(nn.Module):
    def __init__(self, input_size, hidden_size, output_size, layers=1, bi=False):
        super(GruRNN, self).__init__()
        self.input_size = input_size
        self.hidden_size = hidden_size
        self.output_size = output_size
        self.layers = layers
        self.bi_mul = 2 if bi else 1
        
        self.encoder = nn.Linear(input_size, hidden_size)
        self.gru = nn.GRU(input_size, hidden_size, self.layers, bidirectional=bi)
        self.decoder = nn.Linear(hidden_size * self.bi_mul, output_size)
        self.softmax = F.softmax
        
    def forward(self, x, hidden):
        #embeded = self.encoder(x)
        embeded = x
        #print(embeded.view(-1, 1, self.input_size).size())
        #print(hidden.size())
        gru_output, hidden = self.gru(embeded.view(-1, 1, self.input_size), hidden.view(self.layers * self.bi_mul, -1, self.hidden_size))
        #print(gru_output.size())
        output = self.decoder(gru_output.view(-1, self.hidden_size * self.bi_mul))
        return output, hidden
    
    def init_hidden(self, random=False):
        if random:
            return Variable(torch.randn(self.layers * self.bi_mul, self.hidden_size))
        else:
            return Variable(torch.zeros(self.layers * self.bi_mul, self.hidden_size)) 

input_size = 111
hidden_size = 111
output_size = 111
layers = 2

gRNN = GruRNN(input_size, hidden_size, output_size, layers)

gRNN(Variable(torch.FloatTensor(10000, 111)),
     Variable(torch.FloatTensor(layers, 111)))

class Nget():
    def __init__(self, model, char2vec=None, output_char2vec=None):
        self.model = model
        if char2vec is None:
            self.char2vec = Char2Vec()
        else:
            self.char2vec = char2vec
            
        if output_char2vec is None:
            self.output_char2vec = self.char2vec
        else:
            self.output_char2vec = output_char2vec
            
        self.loss = 0
        self.losses = []
    
    def init_hidden_(self, random=False):
        self.hidden = model.init_hidden(random)
        return self
    
    def save(self, fn="GRU_get.tar"):
        torch.save({
            "hidden": self.hidden, 
            "state_dict": model.state_dict(),
            "losses": self.losses
                   }, fn)
    
    def load(self, fn):
        checkpoint = torch.load(fn)
        self.hidden = checkpoint['hidden']
        model.load_state_dict(checkpoint['state_dict'])
        self.losses = checkpoint['losses']
    
    def setup_training(self, learning_rate):
        self.optimizer = optim.Adam(model.parameters(), lr=learning_rate)
        self.loss_fn = nn.CrossEntropyLoss()
        self.init_hidden_()
        
    def reset_loss(self):
        self.loss = 0
        
    def forward(self, input_text, target_text):
        
        self.hidden = self.hidden.detach()
        
        self.optimizer.zero_grad()
        self.next_(input_text)
        target_vec = Variable(self.output_char2vec.char_code(target_text))
        self.loss += self.loss_fn(self.output, target_vec)
        
    def descent(self):
        if self.loss is 0:
            print(self.loss)
            print('Warning: loss is zero.')
            return
        
        self.loss.backward()
        self.optimizer.step()
        self.losses.append(self.loss.cpu().data.numpy())
        self.reset_loss()
    
    def embed(self, input_data):
        self.embeded = Variable(self.char2vec.one_hot(input_data))
        return self.embeded
        
    def next_(self, input_text):
        self.output, self.hidden = self.model(self.embed(input_text), self.hidden)
        return self
    
    def softmax_(self, temperature=0.5):
        self.softmax = self.model.softmax(self.output/temperature)
        return self
    
    def output_chars(self, start=None, end=None):
        indeces = torch.multinomial(self.softmax[start:end], 1).view(-1)#!!!!!!!!!!!!!!!!!!!!!!!!!11
        return self.output_char2vec.vec2list(indeces)

def apply_punc(text_input, text_output):
    result = ""
    for char1, char2 in zip(text_input, text_output):
        if char2 == "<cap>":
            result += char1.upper()
        elif char2 != "<nop>":
            result += char1 + char2
        else:
            result += char1
    return result


result = apply_punc("t s", ['<cap>', '<nop>', ','])
assert(result == "T s,") 

def extract_punc(string_input, input_chars, output_chars):
    input_source = []
    output_source = []
    for i, char in enumerate(string_input):
        # print(i, char)
        if char.isupper() and len(output_source) > 0:
            output_source.append("<cap>")
            input_source.append(char.lower())
        elif char in output_chars and len(output_source) > 0:
            output_source[-1] = char
        elif char in input_chars:
            input_source.append(char)
            output_source.append("<nop>")
    return input_source, output_source

char2vec = Char2Vec(chars=input_chars, add_unknown=True)
output_char2vec = Char2Vec(chars = output_chars)
input_size = char2vec.size 
output_size = output_char2vec.size

print("input_size is: " + str(input_size) + "; ouput_size is: " + str(output_size))
hidden_size = input_size
layers = 1

model = GruRNN(input_size, hidden_size, output_size, layers=layers, bi=True)
egdt = Nget(model, char2vec, output_char2vec)
egdt.load('Gru_get_1_layer_bi.tar')


def train_():
    learning_rate = 1e-4
    egdt.setup_training(learning_rate)


    ##TRAIN

    model.zero_grad()
    egdt.reset_loss()

    seq_length = 100

    for epoch_num in range(400):
    
        step = 0
        for file_name, source in tqdm(source_gen()):
        #for  source in lines:
        
            for source_ in batch_gen(seq_length, source):
            
                step += 1
            
                input_source, output_source = extract_punc(source_, egdt.char2vec.chars, egdt.output_char2vec.chars)
            
                try:
                    egdt.forward(input_source, output_source)
                    if step%1 == 0:
                        egdt.descent()
                    
                except KeyError:
                    print(source)
                    raise KeyError
            

                if step%400 == 399:
                    clear_output(wait=True)
                    print('Epoch {:d}'.format(epoch_num))

                    egdt.softmax_()
                             
                
                    result = apply_punc(input_source, egdt.output_chars())
                    print(result)
                
def predict_(input_text, gen_length=None, temperature=1):
    
    if gen_length is None: 
        gen_length = len(input_text)
    
    clear_output(wait=True)
    
    
    egdt.init_hidden_()
    egdt.next_(input_text)
    egdt.softmax_()
    output = egdt.output_chars()
    
    #print(output)
    result = apply_punc(input_text, output)
    print(result)
    

train_()    
egdt.save('./Gru_get_1_layer_bi.tar')
#!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!1

#from ipywidgets import widgets
#from IPython.display import display

##USE

predict_(" Håp om Oslo-tur eller ", 100,1)

###!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!

#import re

#results=[]
#for fn, sourse in source_gen():
#     split_regex = re.compile(r'[.|!|?|…]')
#     sentences = filter(lambda t: t, [t.strip() for t in split_regex.split(sourse)])
#     for i,s in enumerate(sentences):
#          text_ = "".join(c for c in s if c not in ('!','.',':', '?', ','))
#          results.append([i,text_, predict_(text_, 100,1)])
#          break
